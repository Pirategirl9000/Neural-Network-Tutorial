<!DOCTYPE html>
<html>
    <head>
        <title>Neural Networks Explained</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <h2>A Comprehensive Look at Neural Networks</h2>
    </head>

    <body>
        <ul>
            <!-- What is an AI -->
            <li>
                <h3>What is an AI</h3>
                
                <p>In order to understand how an AI works we first must consider what an AI actually is</p>

                <p>In its simplest form an AI or Artificial Intellegence is: 
                    <a href="https://www.coursera.org/articles/what-is-artificial-intelligence">
                        Artificial intelligence (AI)
                    </a> 
                    refers to computer systems capable of performing complex tasks that historically
                     only a human could do, such as reasoning, making decisions, or solving problems.
                </p>

                <h4>Early Artificial Intellegence:</h4>
                    <p>
                        Early AI models used a brute force approach to solving problems. A brute force algorithm
                        works by trying every possible action and seeing which works best. They would often follow
                        pre-written instructions were much slower as a result of this. When we think of modern AI
                        we usually discard the idea of a computer trying every possibility in light of recent algorithms.
                    </p>
                

            </li>
            
            <!-- Types of AI -->
            <li>
                <h3>Types of AI</h3>
                <p>
                    AI is a broad range of algorithms and purposes. Thus it is split into many different models
                    which we will discuss here. Before breaking down the different types of AI we must discuss
                    a few vocab words
                </p>

                <h5>Dataset: A collection of data used for analysis</h5>
                <h5>Element: An item within a dataset</h5>
                <h5>Label: The output of an element in a dataset</h5>
                <h5>Feature: The input of an element in a dataset</h5>

                <table>
                    <tr>
                        <th>Color</th>
                        <th>Shape</th>
                        <th>Area</th>
                        <th>Child plays with</th>
                    </tr>
                    <tr>
                        <td>"Blue"</td>
                        <td>"Hexagon"</td>
                        <td>402</td>
                        <td>F</td>
                    </tr>
                    <tr>
                        <td>"Red"</td>
                        <td>"Circle"</td>
                        <td>12</td>
                        <td>T</td>
                    </tr>
                </table>

                <p>
                    Say we want to use the data above to determine whether a kid will want to play
                    with a toy based on color, shape and area. In this example each row would be an
                    element. The color, shape, and area would be the features or input data. Lastly
                    the labels would be whether the child plays with the toy.
                </p>

                <p>
                    Now that we understand the vocabulary, let's move onto different types of core learning algorithms
                    used with AI
                </p>

                <ol>
                    <li>
                        <h4>Unsupervised Learning</h4>

                        <p>
                            Unsupervised learning is a method of AI which learns without human intervention. These models
                            are given data and allowed to cluster the data together by similarity without providing labels
                            for the data
                        </p>
                    </li>
                    <li>
                        <h4>Supervised Learning</h4>

                        <p>
                            Supervised learning is a method of AI which learns by providing features and labels.
                            This is the method used with neural networks and nearly all artificial intellegence.
                        </p>
                    </li>
                    <li>
                        <h4>Reinforcement Learning</h4>

                        <p>
                            Reinforcement learning is the most unique of the three main learning algorithms.
                            This method uses the psychological principles of reinforcement and punishment to teach the model.
                            This can be done by rewarding actions that bring the AI closer to the right answer and punishing them
                            for making wrong moves.
                        </p>
                        <button id="redirect_dino_game_ai">View an Example</button>
                    </li>
                </ol>
            </li>

            <!-- What is a Neural Network -->
            <li>
                <h3>What is a Neural Network</h3>

                <p>
                    Neural Networks are center-stage when it comes to modern AI and it's easy to see why.
                    These models can process data and make predictions with swiftness remeniscent of magic.
                    However, under the hood, these models are just using simple mathematics
                </p>

                <canvas id="neural network visual 1" width="720px"></canvas>
                <script>draw_neural_network("neural network visual 1");</script>

                <p>
                    Here we can see a visual representation of a neural network. The neural network is comprised
                    of nodes(seen as circles) connected by edges(the lines connecting). Each node, sometimes called
                    neurons, falls under a layer and has it's own set of instructions. Some layers may have more than
                    one node but for simplicity we'll only draw one node per layer
                </p>

                <p>
                    Neural Networks use supervised learning to develop their own patterns between features and labels
                    This means we give the model the input data and allow it to manipulate the data to create a prediction
                    which it will then compare with the actual label to determine its accuracy. After determining its
                    accuracy, it will then go back through and adjust itself to be more accurate.
                </p>

                <h4>Weights, Biases, and Activations</h4>

                <p>
                    Weights, Biases, and Activation functions are the core mechanisms of neural networks. Each node will 
                    have its own weight, bias, and activation which manipulate the data in some way to help produce a prediction
                </p>

                <p>
                    <strong>As data gets passed between nodes it will be multiplied by a weight value, then a bias is added,
                    and finally it will be put through an activation function of some sort.
                    This altered data will then be passed to the next node and undergo the same process 
                    until the data reaches an output layer. This process is called <i>forward propagation</i></strong>
                </p>

                <p>
                    Don't worry if you don't understand it yet, hopefully an example will help to cement this process.
                </p>

                <p>
                    The neural network we will discuss here is called a multi-layer perceptron(MLP). There are also single-layer perceptrons which
                    differ from MLP due to the lack of hidden layers and only using step functions. The third type of neural network is called a 
                    Deep Neural Network which are simply a scaled up version of MLP's having hundreds or thousands of hidden layers. 
                    Below is a picture of the three different neural network models.
                </p>

                <img src="https://www.researchgate.net/profile/Ahlam-Mallak/publication/351146240/figure/fig5/AS:1017853977305094@1619686707440/Comparison-Between-Perceptron-Multi-layer-Perceptron-and-Deep-Neural-Network.png">
                <a href="https://www.researchgate.net/figure/Comparison-Between-Perceptron-Multi-layer-Perceptron-and-Deep-Neural-Network_fig5_351146240">Source of Photo</a>
            </li>

            <!-- Forward Propagation -->
            <li>
                <h3>Forward Propagation</h3>

                <p>
                    Now we get into the math of AI. The first step is called forward propagation or feed-forward since we move
                    forward through the network. Think of it like a game of telephone where we have one person who sends a message
                    to the next person and it gets changed along the way until we reach the final person. Except with a neural network,
                    we want the initial message to change.
                </p>

                <p>
                    Whenever thinking of Neural Networks I like to think of the formula for a straight line: y = mx + b. To refresh your
                    memory. The formula says that the y value of a point is equal to the slope(m) times the x plus the y-intercept(b).
                    In Neural Networks we have <i>weights</i> which get multiplied by the input and then <i>biases</i> are added, just like
                    in the formula for a line.
                </p>

                <p>
                    These weights and biases are used to manipulate the data to bring us closer to a prediction. Let's do a thought experiment
                </p>

                <p>
                    Let's say you are give the number 2 and you must get it to 13 by multiplying one number and adding one number.
                    What numbers would you use for this?
                </p>
                
                <p>
                    Many would likely say you multiply the 2 by 6 and then add 1 making it thirteen. There are many ways to multiply and add
                    to get to that number but we chose a quick and straight-forward method. A Neural Network might use different weights and
                    biases which would fit a larger range of data.
                </p>

                <p>
                    Lets say we did the same thing but we needed a weight and bias that would allow 2 to become 13 AND 1 to become 6.
                    Now we need to adjust those weights and biases to fit both of these outputs. This is what a Neural Network is doing,
                    often using tens-of-thousands of different inputs or outputs but the same few weights and biases as well as a little help
                    from activation functions
                </p>

                <p>
                    What is an activation function though? Well, simply put, it's a function that modifies the value put into it. It's best
                    if we look at the graph and formulas for a few different activation functions to illustrate this point
                </p>

                                <!--Thx Desmos for easy embedding-->
                <iframe src="https://www.desmos.com/calculator/6mxmlh7gib?embed" width="500" height="500" style="border: 1px solid #ccc" frameborder=0></iframe>
                <h5>Blue: Sigmoid Function  f(x) = 1/(1+e^-x)</h5>
                <h5>Orange: ReLU Function  f(x) = max(0, x)</h5>
                <h5>Red: tanh Function  f(x) = tanh(x)</h5>
                <h6>Click the embedded graph to view directly and to adjust for colorblind friendly use</h6>

                <ul>
                    <li>
                        Using this graph we can see that the sigmoid function will always output a number between
                        0 & 1 never quite reaching either number. In math we call any point where y is undefined
                        horizontal asymptotes. Using this info we can say that the sigmoid function is bounded above and below at 0 and 1
                        giving it a range of (0, 1) 
                        <a href="https://www.varsitytutors.com/hotmath/hotmath_help/topics/interval-notation">Interval Notation</a>
                    </li>

                    <li>
                        The tanh function, also known as the hyperbolic tangent, has horizontal asymptotes at the points -1 and 1. This is 
                        similar to sigmoid but covers a broader range. Using this info we can say that the tanh function is bounded above
                        and below at -1 and 1 and will never reach those values given it a range of (-1, 1)
                    </li>

                    <li>
                        Most unique of the three functions, ReLU; also known as Rectified Linear Units; is {0 when x < 0} and is {x when x >= 0}.
                        This function is bounded below at 0 giving it a range of [0, INFINITY)
                    </li>
                </ul>

                <br>

                <p>
                    Now that we know a few activation functions, what do they do. Well, activation functions help to change the input into
                    something more usable by the network. Lets say we have a neural network for predicting whether a felon will reoffend.
                    We would likely have it go through multiple layers with the goal of a getting either an answer or probability(depending
                    on what we want)
                </p>

                <p>
                    When we get to the output layer we want the activation function to change it into something we can read because before it
                    makes it there it could be any number which wouldn't make any sense. When we're talking about reoffending, what would the 
                    number 1097.85 mean. Nothing!
                </p>

                <p>
                    This is where an activation function like sigmoid might come in. Sigmoid would always place this number in the range
                    (0, 1) which is perfect for this question. We could say that the closer the output is to 1 the more likely to reoffend.
                    In fact, if we multiply it by 10 then we would get the percentage of reoffense. This question is considered a binary classification
                    because we are answering a binary, or two option, question.
                </p>

                <p>
                    How about if we wanted to predict a stocks price any given year. We would do the same thing as we did with the felon question
                    but instead of using a sigmoid activation function we might use something like ReLU or an identity(means nothing is changed)
                    function. This question is considered a linear classification
                </p>

                <p>
                    What about if we wanted to determine if a teacher was bad, good, or average. In this case we might do things a little differently.
                    Often in these sorts of models we use probability distributions which determine how likely any value might be. This is called
                    catagorical classification since we are classifying non-numeric data. The output of a model like this might use a sigmoid to give
                    us probabilities just like a binary classification. Where this differs though is that our input would produce three outputs
                    in the form of a vecors which we won't explain here
                </p>
            </li>

            <!-- Backwards Propagation-->
            <li>
                <h3>Backwards Propagation</h3>

                <p>
                    With all of these concepts of forward propagation we can create a multi-layer perceptron neural network, however it's not accurate yet.
                    In order for a neural network to learn we must first teach it by using labeled data or training data. Basically we give it
                    some inputs to data which we already know the correct answer to. Afterwords we calculate a loss and then perform backwards
                    propagation on the whole neural network.
                </p>

                <p>
                    Let's go through each step in back-propagation
                </p>

                <ol>
                    <li>
                        <h4>Loss</h4>
                        <p>
                            First we must calculate the total loss of the prediction. This can be done in many ways such as mean square error(MSE) or something
                            as simple as subtracting the real value from the prediction. When handling loss across several predictions we often use MSE to get
                            the total loss of all the predictions but when considering one prediction we can simply subtract the real value from the prediction
                        </p>
                    </li>
                    <li>
                        <h4>Derivative of Activation Functions</h4>
                        <p>
                            The most complicated part of neural network lies hidden behind differential calculus but I'll do my best to explain it simply.
                            In calculus there is the concept of a derivative which is the slope of a function at a single point. Typically slope is calculated
                            by doing rise/run across two points on a line but with derivatives we can calculate the slope of a curved line at any given point.
                            This is done by calculating the slope between the target point and another point that is infinitesimally far away from the other point.
                            Essentially if we bring the second point so close to the first one then we can approximate a slope at that point, pretty cool if you're
                            a nerd.
                        </p>

                        <p>
                            But what do derivatives have to do with neural networks? Well many of our activation functions are non-linear meaning they don't have
                            a consistent slope. This necessitates derivatives to find slopes. But why do we need the derivative, well let's go back to that game
                            of telephone analogy. Let's say we have a kid named Henry who has below average hearing. This impairment causes him to change the word
                            or phrase much more than others. Similarly in our neural networks, some layers will cause much more change in the input than others
                            and thus we should treat them all a bit differently. If we have a high derivative then we change those weights more.
                        </p>

                        <p>
                            I won't go too far into the details of derivatives as it isn't necessary to be able to calculate derivatives to make neural networks.
                            Just know that to calculate the derivatives of the activation functions we throw the input from that node into the derivative formula.
                        </p>

                        <h4>Derivatives with respect to weights and biases</h4>
                        <p>
                            The whole point of backwards propagation is to calculate the adjustments necessary for the weights and biases to increase accuracy.
                            To do this, we must work backwards through the network. Below I will the equations for getting the derivatives with respect to weights(dw)
                            and with respect to biases(db).
                        </p>

                        <p>dw3 = loss * h'(x2) * x2</p>
                        <p>dw2 = dw3 * g'(x1) * x1</p>
                        <p>dw1 = dw2 * f'(input) * input</p>
                        <br>
                        <p>db3 = loss * h'(x2)</p>
                        <p>db2 = db3 * g'(x1)</p>
                        <p>db1 = db2 * f'(input)</p>
                        <br>

                        <p>
                            loss calculated in previous step<br>
                            f'(), g'(), h'() are the notation for the derivatives of each function -> f'(x) is the derivative of the function f(x)<br>
                            input is the input at the first layer, x1 is the input for the second layer, and so on.
                        </p>
                    </li>
                </ol>
            </li>

            
        </ul>


        <button id = "redirect_nn_widget">Create your own Neural Network</button>

        <script>initialize();</script>
    </body>
</html>